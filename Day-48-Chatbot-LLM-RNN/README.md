# Day 48: Chatbot with LLM/RNN ğŸ¤–

A conversational AI chatbot powered by DialoGPT, a large language model trained on 147 million Reddit conversations!

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Flask](https://img.shields.io/badge/Flask-Web%20Framework-green.svg)
![Hugging Face](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Transformers-yellow.svg)

## ğŸ¯ What This Project Does

This project implements an **end-to-end conversational chatbot** using a Large Language Model (LLM):

- **DialoGPT**: GPT-2 based model fine-tuned for dialogue
- **Context Awareness**: Maintains conversation history for coherent responses
- **Autoregressive Generation**: Generates text token-by-token
- **Real-time Chat**: Interactive web interface with Flask

## ğŸ§  How It Works

### Seq2Seq Architecture (Simplified)

Traditional seq2seq models use:
```
Encoder â†’ Context Vector â†’ Decoder
```

DialoGPT uses a **decoder-only transformer** (like GPT-2):
```
[History + User Input] â†’ DialoGPT â†’ [Generated Response]
```

### The Generation Process

1. **Tokenization**: User message â†’ Token IDs
2. **Context Building**: Concatenate with conversation history
3. **Generation**: Autoregressive token-by-token generation
4. **Decoding**: Token IDs â†’ Human readable text

### Key Parameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| `max_new_tokens` | 150 | Maximum response length |
| `temperature` | 0.7 | Controls randomness |
| `top_k` | 50 | Limits vocabulary choices |
| `top_p` | 0.95 | Nucleus sampling threshold |
| `no_repeat_ngram` | 3 | Prevents repetition |

## ğŸš€ Features

- ğŸ¤– **DialoGPT-medium** - 345M parameter conversational model
- ğŸ’¬ **Context Memory** - Maintains conversation history
- ğŸ”„ **Autoregressive** - Token-by-token generation
- ğŸ›ï¸ **Controllable** - Temperature, top-k, top-p sampling
- ğŸŒ **Web Interface** - Modern chat UI
- ğŸ“Š **Stats Dashboard** - Model info and message count

## ğŸ“¦ Installation

1. **Navigate to the project folder:**
   ```bash
   cd Day-48-Chatbot-LLM-RNN
   ```

2. **Create a virtual environment:**
   ```bash
   python -m venv venv
   ```

3. **Activate the virtual environment:**
   
   Windows:
   ```bash
   venv\Scripts\activate
   ```
   
   Linux/Mac:
   ```bash
   source venv/bin/activate
   ```

4. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

5. **Run the application:**
   ```bash
   python app.py
   ```

6. **Open in browser:**
   ```
   http://localhost:5000
   ```

> âš ï¸ **Note**: First run will download the model (~1.4GB for medium). Be patient!

## ğŸ—ï¸ Project Structure

```
Day-48-Chatbot-LLM-RNN/
â”œâ”€â”€ app.py                # Main Flask app with DialoGPT
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ README.md            # Documentation
â”œâ”€â”€ .gitignore           # Git ignore file
â”œâ”€â”€ venv/                # Virtual environment
â””â”€â”€ templates/
    â””â”€â”€ index.html       # Chat interface
```

## ğŸ”¬ DialoGPT Model Variants

| Model | Parameters | Size | Speed | Quality |
|-------|------------|------|-------|---------|
| DialoGPT-small | 117M | ~500MB | âš¡ Fast | Good |
| DialoGPT-medium | 345M | ~1.4GB | ğŸ”„ Balanced | Better |
| DialoGPT-large | 762M | ~3GB | ğŸ¢ Slow | Best |

To change the model, edit `app.py`:
```python
chatbot = ConversationalChatbot(model_name="microsoft/DialoGPT-large")
```

## ğŸ® API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Chat interface |
| `/chat` | POST | Send message, get response |
| `/clear` | POST | Clear conversation history |
| `/history` | GET | Get conversation history |
| `/stats` | GET | Get model statistics |

## ğŸ§ª Example Usage

### Chat Request
```bash
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello! How are you?"}'
```

### Response
```json
{
  "success": true,
  "response": "I'm doing great, thanks for asking! How about you?",
  "confidence": 0.65,
  "model": "DialoGPT-medium"
}
```

## ğŸ“Š Comparison: RNN vs Transformer

| Feature | RNN/LSTM | Transformer (DialoGPT) |
|---------|----------|------------------------|
| Architecture | Recurrent | Self-Attention |
| Parallelization | âŒ Sequential | âœ… Parallel |
| Long-range deps | ğŸ˜ Limited | âœ… Excellent |
| Training time | ğŸ¢ Slow | âš¡ Fast |
| Context window | ~100 tokens | 1024 tokens |

### Why DialoGPT over RNN?

1. **Better Context**: Attention mechanism captures long-range dependencies
2. **Pre-trained**: 147M Reddit conversations = rich conversational knowledge
3. **Quality**: More coherent, contextual responses
4. **No Training**: Ready to use out-of-the-box

## ğŸ“ Learning Outcomes

By building this project, you'll learn:

1. **Transformers** - How decoder-only models generate text
2. **Autoregressive Generation** - Token-by-token prediction
3. **Conversation Context** - Maintaining chat history
4. **Sampling Strategies** - Temperature, top-k, top-p
5. **Hugging Face** - Using pre-trained models

## ğŸ”§ Customization

### Adjust Response Style

```python
# More creative responses
output_ids = self.model.generate(
    bot_input_ids,
    temperature=1.0,      # Higher = more random
    top_p=0.9,           # Nucleus sampling
)

# More focused responses  
output_ids = self.model.generate(
    bot_input_ids,
    temperature=0.3,      # Lower = more deterministic
    top_k=10,            # Fewer choices
)
```

### Change Model
```python
# Smaller, faster model
chatbot = ConversationalChatbot("microsoft/DialoGPT-small")

# Larger, better model (needs more RAM)
chatbot = ConversationalChatbot("microsoft/DialoGPT-large")
```

## ğŸ”— Related Projects

- **Day 46**: Rule-Based Chatbot (Pattern Matching)
- **Day 47**: FAQ Chatbot with Embeddings
- **Day 35**: GPT-2 Text Generation
- **Day 33**: LSTM Text Generator

## ğŸ“ˆ Day 46 â†’ 47 â†’ 48 Evolution

| Day | Approach | How it Works |
|-----|----------|--------------|
| 46 | Rule-Based | Regex pattern matching |
| 47 | Embeddings | Semantic similarity search |
| **48** | **LLM** | **Autoregressive generation** |

## ğŸ“ License

This project is part of the 100 Days of AI challenge.

---